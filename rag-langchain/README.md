# RAG LangChain

A Retrieval-Augmented Generation (RAG) system built with LangChain for document indexing and intelligent querying. This implementation demonstrates how to build a knowledge retrieval system using PDF documents and vector search capabilities.

## Features

- **PDF Document Processing**: Load and process PDF documents for indexing.
- **Vector Embeddings**: Generate embeddings using OpenAI's `text-embedding-3-large` model.
- **Vector Database**: Store and retrieve embeddings using the Qdrant vector database.
- **Intelligent Chunking**: Split documents using `RecursiveCharacterTextSplitter` with overlap.
- **Similarity Search & Reranking**: Employs a two-stage retrieval process with initial similarity search followed by Cohere-based reranking for enhanced relevance.
- **Context-Aware Responses**: Generates answers using the refined context from reranked documents and the `gpt-5-nano-2025-08-07` model.

## Architecture

### Core Components

- **`indexing.ts`**: Handles document processing and the creation of the vector store.
- **`retrieval.ts`**: Provides an interactive CLI for querying indexed documents, incorporating the reranking logic.

### RAG Workflow

1. **Document Loading**: The system loads a PDF document using `PDFLoader`.
2. **Text Splitting**: Documents are chunked into smaller pieces with a specified size and overlap to maintain context.
3. **Embedding Generation**: Vector embeddings are created for each chunk using OpenAI's `text-embedding-3-large` model.
4. **Vector Storage**: The generated embeddings are stored in a Qdrant collection for efficient retrieval.
5. **Query Processing**: User queries are converted into embeddings to perform a similarity search.
6. **Initial Retrieval**: The system retrieves the top 5 most relevant document chunks based on similarity.
7. **Reranking**: The retrieved chunks are reranked using Cohere's `rerank-english-v3.0` model to improve relevance, selecting the top 3.
8. **Response Generation**: Contextual answers are generated by `gpt-5-nano-2025-08-07` using the refined, reranked information.

## Configuration

### Environment Variables
```env
OPENAI_SECRET_KEY=your_openai_api_key_here
QDRANT_URL=your_qdrant_instance_url
COHERE_API_KEY=your_cohere_api_key_here
```

### Document Processing Settings
- **Chunk Size**: 1000 characters per chunk
- **Chunk Overlap**: 200 characters between chunks
- **Embedding Model**: `text-embedding-3-large`
- **Reranker Model**: `rerank-english-v3.0`
- **LLM for Generation**: `gpt-5-nano-2025-08-07`
- **Collection Name**: `rag-langchain`

## Usage

### Prerequisites
- Node.js 18+
- OpenAI API key
- Cohere API key
- Qdrant vector database instance
- A PDF document named `geeta.pdf` in the root directory.

### Document Indexing

First, index your PDF document:

```bash
# From the rag-langchain directory
npx tsx indexing.ts
```

This will:
- Load the PDF document.
- Split it into manageable chunks.
- Generate embeddings for each chunk.
- Store the embeddings in the Qdrant collection.

### Document Querying

After indexing, you can start the interactive query interface:

```bash
# From the rag-langchain directory
npx tsx retrieval.ts
```

### Example Interactions

```
ðŸ¤– RAG Chat initialized. Type your query:
> What is the main theme of the document?

ðŸ“„ Found 3 most relevant documents:

ðŸ¤– Assistant:
Based on the retrieved context from pages 15, 23, and 41, the main theme discusses...
```

## Technical Details

### Embedding Configuration
- **Model**: OpenAI `text-embedding-3-large`
- **Dimensions**: 3072 (the default for the model)
- **Similarity Metric**: Cosine similarity (Qdrant's default)

### Text Splitting Strategy
- **Method**: `RecursiveCharacterTextSplitter`
- **Chunk Size**: 1000 characters
- **Overlap**: 200 characters
- **Preserves**: Document structure and contextual integrity between chunks.

### Similarity Search and Reranking
- **Initial Retrieval**: Retrieves the top 5 most relevant chunks from Qdrant.
- **Reranking**: Selects the top 3 chunks using Cohere's `rerank-english-v3.0` model for the final context.
- **Metadata**: Includes page numbers for source attribution.

## Response Format

The system provides:
- The number of relevant documents found.
- Page number references for each source document.
- Contextual answers based on the retrieved information.
- A clear indication when the context does not contain relevant information.

## Error Handling

- **File Loading**: Gracefully handles missing PDF files.
- **API Errors**: Provides proper error messages for failures with the OpenAI, Cohere, and Qdrant APIs.
- **Empty Queries**: Includes validation for user input to prevent empty queries.
- **Connection Issues**: Manages network errors for external services.

## Extending the System

### Adding New Document Types
1. Import the appropriate LangChain loaders (e.g., `TextLoader`, `CSVLoader`).
2. Update the loader configuration in `indexing.ts`.
3. Adjust text splitting parameters if needed for the new document format.

### Customizing Retrieval
- Modify similarity search parameters, such as the `k`-value for initial retrieval.
- Adjust reranking parameters, like the `topN` value for the reranker.
- Experiment with different embedding or reranking models to suit your needs.

### Advanced Features
- **Multi-document Support**: Index multiple documents in the same collection.
- **Metadata Filtering**: Add filters based on document metadata.
- **Conversation Memory**: Maintain chat history for follow-up questions.
- **Re-ranking**: Implement secondary ranking of retrieved results

## Performance Considerations

### Indexing Performance
- Large PDFs may take time to process
- Consider batching for multiple documents
- Monitor embedding API rate limits

### Query Performance
- Similarity search is typically sub-second
- Consider caching for repeated queries
- Optimize Qdrant configuration for your use case

## Limitations

- **Single Document**: Currently configured for one PDF file
- **Static Collection**: No dynamic document updates
- **Basic Chunking**: Uses character-based splitting only
- **Single Session**: No conversation history persistence
- **Language**: Optimized for English text processing

## Future Enhancements

- Multi-document indexing and querying
- Dynamic document updates and re-indexing
- Conversation history and context awareness
- Advanced chunking strategies (semantic, paragraph-based)
- Document metadata enrichment and filtering
- Performance metrics and query analytics